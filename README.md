# Final Year Project: Goal Conditioned Reinforcement Learning -- Nanyang Technological University  
Latest updates from my Final Year Project dedicated at **Solving Sparse Reward Problem with Goal Conditioned Reinforcement Learning.** that I have completed during my Bachelors Degree at time at Nanyang Technological University. This is a year long-project done under the supervision of Professor Arvind Easwaran and Dr. Arambam James Singh. The following was the research objective given:

> Goal-conditioned reinforcement learning(GCRL) is a sub-field of RL where the objective is to train an agent to achieve multiple goals in an environment. The problem setting in GCRL is slightly different from a traditional RL setting where the agent only learns to maximize a long-term reward, such setting is considered to be a single goal setting. However, in GCRL with multiple goals, our aim is to develop an agent which can learn a more generalized behavior(or diverse skills) for the distribution of goals.

After a year of work, the conclusion of the project yielded positive results in the production of a novel algorithm for sub-goal generation to increase training efficiency in reward sparse environments. This new algorithm is not included in this doc for confidentially matters. However, it will be included now. This document contains a background about Goal Conditioned Reinforcement Learning and the problem of Sparse Rewards. Note that if you are a beginner to Reinforcement Learning, it will benefit you to know about basics of Reinforcement Learning before. There are countless learning resources available. Many of them are free too. I have also written a blog on Reinforcement Learning on my website that you can check out too. The following section describes the Sparse Reward Problem and the Goal Conditioned Reinforcement Learning.  


Standard Reinforcement Learning is adept at playing games, solving mazes, etc. However, as the environments get increasingly complexed, these algorithms become too inefficient. Now what does a complex environment mean? There are many definitions of complex environments. For example, a complex environment can be characterized by the large state space. There are too many positions for the agent to be in, requiring each state to have a positive meaning. Another way to classify complex environments is through stochasticity. This is when the output of an agent's actions produces unexpected results. However, in this article, when I refer to complex environment, I will not be refering to these characteritistcs. In this article, a complex environment will be defined only by the reward sparsity(the ratio of states in an environment that yield a reward to the agent to states that yield no reward). In a typical sparse reward environment, there is only 1 state that provides a reward and this goal is too far away. Let's consider the following environment. In this environment, the agent has to arrive at the goal. If you already have experience with RL, you will understand how difficult it's for standard RL to arrive at this goal. But let me just explain once again. Standard RL depends on exploring the goal through exploring random moves. I want to put emphasis on the plurarity of moves. this is because to arrive at a goal, an agent must exhibit a specific set of moves. Not only that but these moves need to be in the correct order. So we rely on producing random moves that get thr agemt to its requried state. tje probability of this happening is veru low. While the premise of RL states that random moved help it to discover its goal, this is a very unstable process that relies on chance and luck. As environments become too reward sparse, the solution worsens. The chance to make the correct moves decreases. 

Unfortunately real life scenarios demand are rewaed sparse. Real life tasks are concerned with meeting a goal. either you meet a goal or you don't. for example, either a robot picks up box or it does not. player solved the maze or it foes not. there is no in between. So we have to either get there or we don't. 